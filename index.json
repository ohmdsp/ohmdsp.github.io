[{"content":"Hello! I’m the developer and author of OhmDSP.\nI’m a scientist/engineer working in defense tech on sensor signal processing systems and solutions. I have a BSc degree in Physics and an MSc and PhD in Electrical Engineering. I primarily use this blog to share concepts and info about projects on my Github site. Hopfully, other humans can get something useful from the content I post here.\n","description":"Telling my story","tags":null,"title":"About Me","uri":"/about/"},{"content":"","description":"","tags":null,"title":"Books","uri":"/tags/books/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"},{"content":"","description":"","tags":null,"title":"Lesson-Path","uri":"/categories/lesson-path/"},{"content":"This is my list of reading suggestions based on what I have found educational and enjoyable. I will continue to update the list as I discover more treasures. Warning: I primarily like to read non-fiction.\n1. Seven Brief Lessons on Physics, by Carlo Rovelli This is a well-written book that is both educational and enjoyable. It is a short book, so it can be read in a weekend. I highly recommend it for anyone interested in modern physics.\n2. Reality Is Not What It Seems: The Journey to Quantum Gravity, by Carlo Rovelli\nI really enjoyed the background material and physics history presented in this book. It felt like a fresh approach, and the historical connections were very enlightening. The later sections of the book dealing with quantum loop gravity were interesting but less informative than other sections, in my opinion. During one chapter, there is an awkward attempt made to discuss God and religion, which seems hard to resist for physics writers even though it is surely outside their expertise. In summary, this book is worth owning and possibly reading a couple times.\n3. The Signal And The Noise, by Nate Silver\nThis book is packed with information and stories related to statistical analysis of world events, weather patterns, natural disasters, markets, gambling, etc. I found it to be too wordy throughout so I skimmed through some sections in order to stay interested. However, it is worth reading if you want to know about statistical prediction applied to many different modern history events. If you want to learn about the actual math behind statistical prediction, you will need to get a different book. The sections on games of chance are very interesting.\n4. The Theoretical Minimum, by Leonard Susskind This is a good book for brushing up on your basic physics. I put it in my re-read list since it never hurts to go over it again. I am looking forward to reading and studying Dr. Susskind’s other books on quantum mechanics and relativity. If you have already read Feynman’s Lectures on Physics, then this is a good next step.\n5. Thinking Fast and Slow, by Daniel Kahneman If you like psychology and the study of human decision making, you will likely enjoy this book. I did not find it to be a quick read and for me, it required several weeks to digest piece-by-piece. At times, the book feels like a random assortment of stories and studies, so don’t expect an organized narrative. There is a lot of interesting information about human thinking and decision bias presented in the book. It will have you re-evaluating some of your decision making skills.\n6. The Radioactive Boy Scout, by Ken Silverstein I loved this short book! The story is both inspiring and sad. The book presents a lot of general information about radioactivity and physics in an easy-to-read manner. I highly recommend this book to anyone interested in physics or chemistry. It is a great book to share with your teenage son or daughter.\n7. Feynman’s Lectures On Physics, by Richard Feynman I recently revisited these lectures for some self-study. I was reminded how good they are. I will admit, than sometimes the conversations get long and wordy, but Dr. Feynman’s thought process is enlightening. Plan to spend many weeks, maybe months, reading/studying these lectures.\n8. Digital Communications, by Andy Bateman This is meant to be a textbook, but it is short and very well written. I highly recommend this book to any systems engineers looking to get a high-level overview on digital communications. The graphics are also very good.\n9. The Perfectionists, by Simon Winchester\nA wonderful book about engineering and precision. I really loved the historical aspect of this book along with stories of impressive engineering.\n10. Astrophysics for People in a Hurry, by Neil DeGrasse Tyson\nAn informative quick read perfect for a Sunday afternoon. This book contains simple descriptions of important discoveries and concepts around the Cosmic Background Radiation, Dark Matter, Dark Energy, etc. This would be a good book to share with a young student.\n11. Vibrations and Waves in Physics, by Iian G. Main\nThis book is actually intended as a short textbook for a physics course. However, it is perfect for self-study or refreshing knowlege from your undergraduate physics courses. Keep in mind that a solid understanding of basic physics and differential equations is needed to get the most out of this book.\n12. Feynman Lectures On Computation, by Richard Feynman\nThis is a fantastic book and a great addition to the lectures listed above. Consider this book a complete introduction to logic operations, encoding and the physics of computation. A book worth reading multiple times.\n13. The Perfectionists, by Simon Winchester\nAlthough I found the first few chapters of this book to be a little boring and slow, the last several chapters are wonderful and insightful. If you have an interest in the history of navigation, time keeping and how modern scientific measurements have influenced our lives, this book is worth the time to read.\n14. The Stars, by H.A.Rey\nThis book is amazing! Yes, it is written by author of the many popular Curious George books. If you have any interest in the stars and celestial mechanics, you need this book. The drawings and diagrams are incredibly insightful. The sections on the celestial sphere, orbits and the seasons are easy to read and understand. Buy this book!\n15. Ultralearning, by James Young\nAn interesting book with some insightful stories and lessong about people that have mastered a topic or skill thorugh intense effort combined with ability. Like many books written around self imrpovement, some sections are repedetive and too verbose. However, it is not difficult to quickly scan these sections. Worth a quick read if you are interested in learning a something challenging and are not sure how to start.\n16. On Tyranny, by Timothy Snyder\nA short read with some interesting insights and historical information regarding fascism and communist tyranny. The author spends too much time comparing the outcome of the 2016 election and president elect to the rise of fascism, but when he stays on topic the reading is insightful. I recommend this book for the historical information only.\n17. Ice Age, by John and Mary Gribbin\nIf you are interested in the scientific theories and experiments developed over the past century in order to better understand the long term history of Earth’s climate, then you will enjoy this book.\n18. How To Fly A Horse, by Kevin Ashton\nA fun book about the history of invention and discovery. Some chapters will be more interesting than others depending on your interests and background.\n19. Chasing the Demon, by Dan Hampton\nAn interesting book about aviation history and the milestones that led up to flying faster than the speed of sound. The book also includes some interesting photographs from the post WWII era when secret flight tests were being made in the US.\n20. Rocket Billionaires, by Tim Fernholz\nI really enjoyed this book. It includes a readable history of the various commercial space companies led by the well-known billionaires of today. The stories about SpaceX alone are worth the price of the book. Reading this book actually increased my interest in the modern commercial space race and the companies involved.\n21. Zero to One, by Peter Thiel\nIn my opinion, one of the best startup books out there. Read it!\n22. Lost In The Math, by Sabine Hossenfelder\nThis is a refreshing look at the progress in particle physics. Sabine is very engaging and brings a new perspective to the topic. She also authors a blog and puts out some really educational YouTube videos.\n23. Unsettled, by Steven Koonin\nA very interesting book about the difficulties and uncertainties in the science of climate change. The author is a qualified physicists with many years of experience in the research and the politics behind climate issues. At times the book was a little unorganized, but the content was well worth the time it takes to read.\n24. The Hunter Killers, by Dan Hampton\nThis is a fascinating book about the Vietnam War and methods used to hunt and destroy SAM missile installations in North Vietnam. Sections of the book are packed with adrenaline infused stories of actual missions. The book has all sorts of interesting information about the strategies and technologies used on both sides. A very enjoyable book!\n25. The Operator, by Dan Robert O’Neill\nIntense, eye-opening, interesting. Read it!\n26. The Way Of The Knife, by Mark Mazzetti\nAn eye-opening education about how the CIA and DoD conduct war and how it has evolved over the past 25 years. This book contains an impressive amount of information.\n26. David and Goliath: Underdogs, Misfits, and the Art of Battling Giants, by Malcolm Gladwell\nI know I am late to the party when it comes to this book, but I finally got around to reading it and must admit that is was interesting. The sections about the potential perils for some when selecting a top university were especially interesting. This is an interesting book, even if it sometimes seems to suffer from its own confermation bias.\n27. Titan, by Ron Chernow\nA very detailed and interesting book about the life of John D. Rockerfeller, Sr. As an entrepreneur it was educational to read about his life and decision processes. It is a thick book, but the account of post civil war industrial development made this book worth heavy cost of time it took to read. There are also many lessons about business and investing worth learning.\n28. Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future, by Ashlee Vance\nThis is a good book that will both excite and distrurb the reader. I found it interesting and insightful as an entrepreneur. It was especially useful in triggering deeper thinking about what it took for Elon to be succesful at extremely large efforts. Both the good and bad are layed bare in the book.\n29. Einstein: His Life and Universe, by Walter Isaacson\nI really enjoyed this biography. In addition to some nice sections on physics, I leared several new things about Eistein the person. It was interesting to learn that he made and published mistakes, struggled with several relationships, and resisted the interpretations of Quantum Mechanics held by other famous physicists.\n30. Grant, by Ron Chernow\nThis is a very impressive book. It took me a lot of time to complete, but I leanred a lot about both Grant and the Civil War era that I did not previously know. I came away with a greater appreciation for Grant and the struggles he faced during his lifetime. I also gained a better appreciation for that very important time in US history. Read this book!\n30. Steve Jobs, by Walter Isaacson\nI read this over the winter months in late 2021. It was really interesting to read about Steve Jobs youth and family life. His path to success is unique along with his approach to life. I enjoyed learning more about the beginnings of Apple and the people involved. Steve certainly has some flawes as a person, but he was a force for innovation. The book starts to get a little too detailed and long in the second half. I admit to skipping a few sections that I did not find very interesting. Overall, this is a good book and worth the investment in time to read.\n","description":"Descrizione da rivedere se è un doppione subtitle","tags":["Books","Reading"],"title":"My Reading Recommendations","uri":"/my-reading-recommendations/"},{"content":"","description":"","tags":null,"title":"Posts","uri":"/posts/"},{"content":"","description":"","tags":null,"title":"Reading","uri":"/tags/reading/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"},{"content":"","description":"","tags":null,"title":"DSP","uri":"/tags/dsp/"},{"content":"","description":"","tags":null,"title":"Fourier Transform","uri":"/tags/fourier-transform/"},{"content":"One of the most important concepts in digital signal processing is that a Linear-Time-Invariant (LTI) system is completely characterized in the time-domain by the output response to a unit impulse input. This can be imagined as probing the input of an LTI system with a Kronecker delta function and then determining the resulting output. $$ \\delta[n] \\rightarrow \\boxed{LTI} \\rightarrow h[n] $$\nIn this simple diagram, the output function $h[n]$ is called the “impulse response” of the system.\nLet’s imagine that we want to experiment with our LTI system by sending in other kinds of discrete-time sequences (i.e. signals). For example, a short sequence could be $$ s[n] = x[1]\\delta[n-1] + x[2]\\delta[n-2] + x[7]\\delta[n-7] $$\nwhere the $x[k]$ terms are the amplitudes of each time-delayed unit impulse.\nMore generally, any arbitrary discrete-time sequence can be expressed as a weighted superposition of time-shifted unit impulses, or $$ s[n] = \\sum^{\\infty}_{k=-\\infty}x[k]\\delta[n-k] $$\nWe can continue our experiment by sending this arbitrary sequence into our LTI system and then determining the output based on the rules we know about LTI systems. For example, if we input a single scaled and time-delayed impulse, we would get a scaled and time-delayed unit impulse response at the output $$ x[k]\\delta[n-k] \\rightarrow \\boxed{LTI} \\rightarrow x[k]h[n-k] $$\nUsing the same rules, when we input the full arbitrary sequence, we end up with $$ \\sum^{\\infty}_{k=-\\infty}x[k]\\delta[n-k] \\rightarrow \\boxed{LTI} \\rightarrow y[n] $$\nwhere the output can be expressed by $$ y[n]=\\sum^{\\infty}_{k=-\\infty}x[k]h[n-k]$$\nLetting $k = n-k$ and rewriting this output, gives us $$ y[n] = \\sum^{\\infty}_{k=-\\infty}x[n-k]h[k] $$\nwhich is the well-known convolution sum! Our experiment has shown us that if we input any arbitrary signal into an LTI system, the output will be the convolution of the input signal with the system impulse response, or $$ x[n] \\rightarrow \\boxed{LTI} \\rightarrow y[n] = x[n] * h[n] $$\nwhere $ x[n] * h[n] $ is a shorthand way of expressing the convolution operation.\nThis is certainly a very interesting result considering all we have done so far is experiment with input signals made up of scaled and delayed unit impulses. What would happen if we used a different input? How about using a complex exponential as the input? $$ x[n] = e^{j\\omega n}$$\nRecall again that the output of an LTI system due to an input $x[n]$ is just the convolution of the input and the system impulse response, or $$ x[n] \\rightarrow \\boxed{LTI} \\rightarrow y[n] = \\sum^{\\infty}_{k=-\\infty}x[n-k]h[k]$$\nWhen $x[n] = e^{j\\omega n}$ the output will be $$ y[n] = \\sum^{\\infty}_{k=-\\infty}h[k]e^{j\\omega(n-k)} $$\nwhich can be rearranged to look like $$ y[n] = e^{i\\omega n} \\sum^{\\infty}_{k=-\\infty}h[k]e^{-j\\omega(k)} $$\nLet’s write the summation term as a function $H(e^{j\\omega})$. Then we can rewrite the output like this $$ e^{j\\omega n} \\rightarrow \\boxed{LTI} \\rightarrow H(e^{j\\omega}) e^{i\\omega n} $$\nNow, take a minute to notice that although we input a complex exponential to our LTI system, we got out the same complex exponential multiplied by a complex scaling term! The scaling term is called the “frequency response” or “spectrum” and it can be written as $$ H(e^{j\\omega}) = \\sum^{\\infty}_{k=-\\infty}h[k]e^{-j\\omega k} $$\nWhen we let k=n this becomes the equation for the Discrete-Time-Fourier-Transform, which is an important theoretical tool for understanding digital signal processing. We can write the DTFT as $$ H(e^{j\\omega}) = \\sum^{\\infty}_{n=-\\infty}h[n]e^{-j\\omega n} $$\nAnother thing to notice is that by using a complex exponential as the input to an LTI system, we turned convolution into multiplication. The complex spectrum can be computed for any signal by $$ X(e^{j\\omega}) = \\sum^{\\infty}_{n=-\\infty}x[n]e^{-j\\omega n} $$\nIn practical computer applications, we do not use the DTFT (which has a continuous output) but instead use the DFT (which has a discrete output). For example, if we make our signal sample indices to be $ 0 \\leq n \\leq N-1 $ and uniformly sample the continuous output $X(e^{j\\omega})$ at N equally spaced index points (or frequencies) between $0 \\leq k \\leq N-1$, then $\\omega = 2\\pi k/N$ and we get an expression for the DFT as $$ X[k] = \\sum^{N-1}_{n=0}x[n]e^{-j\\frac{2\\pi kn}{N}} $$\nKeep in mind that $X[k]$ is complex valued even when $x[n]$ is real valued.\nNow, let’s continue with our experiments and figure out what would happen if we input a more general complex value into our LTI system? Let $ x[n] = z^{n}$ where $z = re^{j\\omega n}$. This is a general complex exponential with a variable amplitude. Again, we exploit our knowledge of LTI systems and convolution to arrive at $$ z^{n} \\rightarrow \\boxed{LTI} \\rightarrow \\sum^{\\infty}_{k=-\\infty}h[k]z^{n-k} $$\nAgain, we can simplify the right side to be $$ y[n] = z^{n} \\sum^{\\infty}_{k=-\\infty}h[k]z^{-k} $$\nJust like the previous experiment when we put in a complex exponential, we can see that the output is just the input multiplied by a complex scaling term. We can express this output as $$ y[n] = z^{n}H(z) $$\nwhere $H(z)$ is called the “transfer function” of the system. In general, the transfer function of a any signal $x[n]$ is computed by $$ X(z) = \\sum^{\\infty}_{n=-\\infty}x[n]z^{n} $$\nThis is called the z-transform! A cool fact is that the z-transform can be used to analyze unstable systems. Also, notice that the z-transform reduces to the DTFT when $r=1$.\nOk. We have arrived at a few very important DSP mathematical tools (e.g., convolution, DTFT, DFT, and z-transform) just by running experiments that look at the output of an LTI system given different input signal types. These powerful mathematical tools form the foundation of digital signal processing theory. In a future post, I will dive deeper into how these tools can be used, and I will introduce the inverse transforms associated with the DTFT, DFT and z-transform.\n","description":"Descrizione da rivedere se è un doppione subtitle","tags":["Fourier Transform","DSP","Z-Transform"],"title":"Intuitive Derivations in DSP","uri":"/intuitive-derivations-in-dsp/"},{"content":"","description":"","tags":null,"title":"Z-Transform","uri":"/tags/z-transform/"},{"content":"","description":"","tags":null,"title":"Non-Stationary","uri":"/tags/non-stationary/"},{"content":"Check out the Matlab demo code to explore time-frequency analysis using a Bat signal…not the super-hero one.\nOften, the examples provided in our signal processing textbooks assume that the signal we are working with is wide-sense-stationary. This means that the first and second order statistics (mean and correlation) do not change with shifts in time. Although, all of our statistical signal processing techniques depend upon this statistical behavior in order to produce meaningful characterizations when analyzing a random signal, many real-world signals don’t have nonstationary characteristics. A bat echo-location signal is just one example.\nHow can we use statistical signal processing techniques to analyze time-changing statistical behavior? We must first determine what is the longest time interval $N$ over which we can assume that the data is approximately stationary, and then step through the random signal (with or without data overlap) using small analysis intervals never exceeding $N$. This is done to accurately capture the quasi-stationary statistical characteristics for each analysis interval. The changing statistical behavior may then be tracked by creating a stacked plot of the statistics estimated each analysis interval. This plot is called a time-vs-frequency spectrogram (Note: other types of time-vs-feature grams are also possible, but not discussed here).\nFrequency-Domain Analysis If the autocorrelation sequence (ACS) for a signal is stationary, then the Fourier transform of the ACS (the power spectral density, or PSD) is also stationary. A way to visualize this is to look at the shape of the plotted PSD or the ACS. If the signal is stationary, these plotted shapes should not change over time. If time varying content is observed, then the random signal has nonstationary statistical behavior. We could experimentally vary the analysis interval duration $N$ until differences in successive PSD estimates are observed, indicating the threshold between quasi-stationary and nonstationary statistical behavior.\nUsing bat echo-location data, an experimental determination was made that $ N = 38 $ samples was the threshold between quasi-stationary and stationary statistical behavior. The time-vs-frequency gram created by using a sample spectrum (magnitude of FFT) for each analysis intervals of 38 samples, with 37 sample overlap (i.e., only one sample shift), is shown in the figure below. Notice the four separate signal components being generated simultaneously by the bat during echo-location.\nFig 1. TF analysis of a bat echo-location signal. The bat data has an approximately linear time-vs-frequency trajectory in the TF analysis gram that makes it possible to calculate an approximation for the longest time interval in which the bat signal can be considered to be quasi-stationary. The Fourier transform of a windowed data segment of duration $ NT $ seconds (where $ N $ is the number of data points and $ T $ is the sampling interval) produces a frequency domain spectrum mainlobe response which is roughly $ 1/NT $ Hz in 3dB bandwidth. Therefore, a criterion for having a roughly stationary statistical behavior is that any change in frequency content be less than $ 1/NT $ Hz over an analysis interval, as this is not resolvable. If this can be made true, the signal is considered to be quasi-stationary over the analysis interval of $ N $ samples. From examining the bat spectrogram, we can see there is an approximately linear change of frequency vs time from 50 KHz at 0.65ms to 32 KHz at 1.8 ms. The slope in this region is $ s=(50-32)/(1.8-0.65)=15.65 $ KHz/ms. Thus, if $ NT $ is expressed in ms, the change in frequency is $ sNT $ KHz. We want this to be less than the mainlobe response bandwidth. So, the threshold between quasi-stationary and nonstationary conditions is approximately $ sNT = 1/NT $. Since the bat data was sampled at $ T = 7 $ microseconds, solving for $ N^2=1/sT^2 $ yields $ N = 36.1 $ samples, which compares well with our experimentally determined value of $ N=38 $.\nTry some of these parameter changes to explore what happens:\nChange the FTT size (example - 64 samples, or 2048 samples) Change the analysis interval duration (example - 8 samples or 256 samples) Change the overlap sample size (example - make same as analysis interval duration, try $1/2$ that size) Make notes on what changes while you experiment Resources Don’t forget to download the code and bat signal data so you can try for yourself - Code \u0026 Bat Data Thanks to Dr. Larry Marple Jr. for the introduction to time-frequency analysis back when I was in graduate school. Please check out Larry’s well-known book on Spectral Analysis\n","description":"Descrizione da rivedere se è un doppione subtitle","tags":["time-frequency-analysis","DSP","non-stationary"],"title":"Time-Frequency Analysis of Non-Stationary Signals","uri":"/time-frequency-analysis-of-non-stationary-signals/"},{"content":"","description":"","tags":null,"title":"Time-Frequency-Analysis","uri":"/tags/time-frequency-analysis/"},{"content":"","description":"","tags":null,"title":"Engineering","uri":"/tags/engineering/"},{"content":"\nGrain Bins and Soy Bean Farming\nHeard Immunity (Sabine H. has become one of my favorite physics channels)\nMaxwells Demon\nArrow of Time\nMaxwells Demon - Relativity\nGeneral Thermodynamics (Watch at 2X speed)\nCarno Cycle\nQuantum Tunneling\nBose Einstein Condensation\nQuantum Signal Processing\nTorque and Gyroscopic Motion\n","description":"Descrizione da rivedere se è un doppione subtitle","tags":["Physics","Engineering","YouTube"],"title":"Fun Physics and Engineering Videos","uri":"/fun-physics-and-engineering-videos/"},{"content":"","description":"","tags":null,"title":"Physics","uri":"/tags/physics/"},{"content":"","description":"","tags":null,"title":"YouTube","uri":"/tags/youtube/"},{"content":"There are two common methods in the literature for mathematically describing narrowband signals. Both of these methods will be encountered in the popular signal processing, digital communications, and radar signal processing books. First approach: Let’s start with a simple real-valued sinusoidal signal with a time-dependent amplitude and phase. This can be expressed by\n$$ x_{c}(t) = a(t)cos(2\\pi f_{0} t + \\theta(t) ) .$$\nWe can rewrite this expression using Eulers Formula to be\n$$ x_{c}(t) = \\Re \\left[ a(t) \\exp(j [2\\pi f_{0} t + \\theta(t)]) \\right] \\ = \\Re \\left[ a(t) \\exp(j 2\\pi f_{0} t) \\exp(j \\theta(t)) \\right] . $$\nLet’s simplify this a little by splitting out the amplitude and phase components and assigning it to\n$$ \\tilde{x}_{c}(t) = a(t) \\exp(j \\theta(t)) .$$\nThen, we can rewrite our expression for $x_{c}(t)$ as\n$$ x_{c}(t) = \\Re \\left[ \\tilde{x_{c}}(t) \\exp(j 2\\pi f_{0} t) \\right] . $$\nNote that $\\tilde{x_{c}}(t)$ contains both the amplitude and phase information for $x_{c}(t)$, and is referred to as the “complex envelope” of $x_{c}(t)$.\nSecond approach:\nThe second appraoch for describing our signal start with expanding our original expression using a trigonametric identity. This give us\n$$ x_{c}(t) = a(t)cos(2\\pi f_{0}t + \\theta(t) ) $$\n$$ x_{c}(t) = a(t)cos(2\\pi f_{0}t)cos(\\theta(t)) - a(t)sin(2\\pi f_{0}t)sin(\\theta(t)) . $$\nAgain, we will simplify using some assignments for the amplitude and phase components.\n$$ \\begin{equation} \\begin{aligned} x_{ci}(t) \u0026= a(t)cos(\\theta(t))\\\\ x_{cq}(t) \u0026= a(t)sin(\\theta(t)) . \\end{aligned} \\end{equation} $$\nThese are called the “in-phase” and “qaudrature” components of the signal, respectively. A final expression for our signal is then found to be\n$$ \\begin{equation} \\begin{aligned} x_{c}(t) \u0026= x_{ci}(t)cos(2\\pi f_{0}t) - x_{cq}(t)sin(2\\pi f_{0}t). \\end{aligned} \\end{equation} $$\nSummary:\nClearly, the represenations from the first and second approaches are related by\n$$ \\begin{equation} \\begin{aligned} \\tilde{x_{c}}(t) \u0026= x_{ci}(t) + j x_{cq}(t) \\end{aligned} \\end{equation} $$\nThe in-phase and quadrature components are, respectfully, the real and imaginary parts of the complex envelope $\\tilde{x_{c}}(t)$.\n","description":"Descrizione da rivedere se è un doppione subtitle","tags":["Fourier Transform","DSP","Z-Transform"],"title":"Complex Envelope","uri":"/complex-envelope/"},{"content":"","description":"","tags":null,"title":"DSP","uri":"/categories/dsp/"},{"content":"","description":"","tags":null,"title":"Convolution","uri":"/tags/convolution/"},{"content":"","description":"","tags":null,"title":"Correlation","uri":"/tags/correlation/"},{"content":"There are two methods for computing the correlation of complex-valued signals. The first one is a time-domain method, and for a stationary process can be computed using the following formula\n$$ r(k) = \\begin{cases} \\frac{1}{N} \\sum^{N-k-1}_{n=0} x(n+k)h^{\\ast}(n) \u0026 \\text{if } 0 \\leq k \\leq N-1 \\\\ r^{\\ast}(-k) \u0026 \\text{if } -(N-1) \\leq k \\lt 0 \\\\ 0 \u0026 \\text{if } elsewhere \\end{cases} $$\nwhere N is the input data record length, x(n) is the input data and h(n) is the kernel, or reference data. The computational time to compute a correlation using this method is directly proportional to the number of samples in the kernel. Therefore, the longer the kernel data record the more time it takes to compute a correlation.\nThe second method for computing a correlation is a frequency domain method called “fast correlation”. Fast correlation uses the Fast Fourier Transform (FFT) to transform the input signal and the kernel signal into the frequency domain and then exploits the following mathematical relationship $$ x(n) \\circledast h(n) = iFFT({X(k)*conj(Y(k))}) $$\nIt is well known that when dealing with kernel lengths that are greater than about 60 samples it is faster, or more efficient, to use the fast correlation method. To the fact that the time it takes to compute the fast correlation is proportional to the logarithm of the number of samples, and therefore, changes slowly as the kernel length increases. Figure 1 shows a computation time comparison between the time-domain and fast correlation methods versus kernel length, or “impulse response length”.\nFig 1. Execution times for fast correlation compared to time-domain correlation [Steven W. Smith,DSP Guide]. It is often useful to look at a pictorial representation of an algorithm. A block diagram showing the processing steps involved in computing the fast correlation is shown in Figure 2. It is worth mentioning that fast correlation is also called “pulse compression” in the radar community. Therefore, it should be no surprise to learn that many commercially available technologies, both hardware and software, are available to implement it.\nFig 2. Block diagram showing the processing steps for computing frequency-domain correlation. The most efficient way to implement fast correlation for very long input data sequences is to use the overlap “scrap” (OVS) algorithm, also known as the overlap-save algorithm. The overlap-save algorithm can be easily understood by looking at the sample sequences x and h in Figure 3. First, the input data sequence is broken up into arbitrary segments of length $N$. Then, each segment is processed using the fast correlation method discussed in the last section. However, special segment overlapping and data discarding rules must be by followed in order to avoid wrap-around pollution in the output sequence. We will not go into an analysis of wrap-around pollution in this report since most DSP textbooks cover it in detail. The OVS algorithm can be implemented by following these steps:\nZero pad the kernel out to the FFT length $(N)$ to avoid wrap-around pollution Add $M-1$ zeros to the beginning of the input data and $N-1$ zeros to the end Break input signal into successive segments of N samples, each segment overlapping the previous segment by $M-1$ samples Compute the FFT of the kernel and complex conjugate (save for reuse) Buffer a segment of input signal and compute the FFT Multiply (element- wise) outputs from steps 4 and 5 Compute the inverse FFT (IFFT) from output of multiply Last $M-1$ samples of each IFFT output are discarded, (front $M-1$ samples discarded from first output block) The remaining samples are used as output Steps 5-9 are repeated until all input segments used Simple Overlap \"Scrap\" algorithm example. Plots from simple example. It is important to note that the last M-1 samples of each IFFT output should be discarded instead of the front M-1 samples. This will help in the implementation by avoiding double buffering at the output of the IFFT. Matlab code for simulating the OVS correlation algorithm is included below.\nUsually we would consider the following factors when deciding the overlap amount for the OVS algorithm.\nFFT Accuracy – the numerical accuracy of fast correlation is dependent on the error introduced by the FFT and IFFT process. For floating point implementations this is usually negligible, but for fixed point processing, significant dynamic range can be lost when using larger transforms. Latency – the fast correlation process extends the group delay by at least N samples. So, the longer the FFT, the longer the latency. In the absence of a benchmark, an overlap factor of 4 to 8 is a good rule of thumb. Since we do not have a lot of flexibility when choosing a kernel size, we will need to choose the length of the FFT to give optimal performance. % Filename: overlap_scrap_corr.m \u003cbr\u003e % Author: drohm \u003cbr\u003e % \u003cbr\u003e % y = output sequence \u003cbr\u003e % x = input sequence \u003cbr\u003e % h = kernel sequence \u003cbr\u003e % N = segment length \u0026 fft length \u003cbr\u003e % Implements overlap-scrap (OVS) \"overlap-save\" correlation. \u003cbr\u003e %========================================================================== %========================================================================== clear all;close all \u003cbr\u003e x1 = [zeros(1,128)+j*zeros(1,128)]; %Input Data \u003cbr\u003e h1 = [ 1 0 1 1 0 1]; %Kernel\u003cbr\u003e Lenx = length(x1); M = length(h1); \u003cbr\u003e x1(25:25+M-1)=h1; %Embed Kernel into nput data\u003cbr\u003e xp = [x1 zeros(1,length(h1)-1)]; %Zero Padded input sequence and Kernel\u003cbr\u003e hp = [h1 zeros(1,length(x1)-1)];\u003cbr\u003e N=8; % Segment Size and FFT Size for OVS \u003cbr\u003e M1 = M-1; L = N-M1;\u003cbr\u003e h = [h1 zeros(1,N-M)];\u003cbr\u003e Leny = Lenx + M -1; %Length of final correlation output \u003cbr\u003e x = [zeros(1,M-1), x1, zeros(1,N-1)]; K = floor((Lenx+M1-1)/(L)); % # of segment sent to correlator\u003cbr\u003e Y = zeros(K+1,N); % Allocate output 2D array of zeros\u003cbr\u003e %%%%% Correlator %%%%%\u003cbr\u003e % These steps perform the correlation using the FFT\u003cbr\u003e for k=0:K \u003cbr\u003e xk = x(k*(N-(M-1))+1:k*(N-(M-1))+N); % Input segments\u003cbr\u003e Y(k+1,:) = ifft( (fft(xk,N).*conj(fft(h,N))),N ); % FFT Correlation\u003cbr\u003e end\u003cbr\u003e Y = Y(:,1:N-(M-1))'; %discard the last (M-1) samples\u003cbr\u003e y = (Y(:))'; % assemble output into 1D array\u003cbr\u003e y = (real(y(M-1:Leny))); %remove extra zeros at end\u003cbr\u003e %%%%% Plotting %%%%%\u003cbr\u003e figure(1)\u003cbr\u003e subplot(4,1,1)\u003cbr\u003e stem(x1);title('Input Data');ylim([0,2])\u003cbr\u003e subplot(4,1,2)\u003cbr\u003e stem(h1);title('Kernel Data');ylim([0,2])\u003cbr\u003e subplot(4,1,3)\u003cbr\u003e stem(y);title('Output (OVS)');ylim([0,5])\u003cbr\u003e ","description":"Descrizione da rivedere se è un doppione subtitle","tags":["Correlation","FFT","Convolution"],"title":"Fast Correlation","uri":"/fast-correlation/"},{"content":"","description":"","tags":null,"title":"FFT","uri":"/tags/fft/"},{"content":"","description":"","tags":null,"title":"Deep Learning","uri":"/tags/deep-learning/"},{"content":"Signal Classification Using Spectrograms and Deep Learning In this post, I will introduce some basic methods for utilizing a Convolutional Neural Network (CNN) to process spectrograms created from Radio Frequency (RF) data. The information in the post was taken from a tutorial my colleagues and I provided to NVidia Corporation in early 2015 that has been used in their enterprise training series for several years. This post links you to a git project that removes the dependency on Nvidia’s DIGITS platform and uses only Keras and Tensorflow. Project Code \u0026 Data\nIntroduction to signal detection When monitoring radio frequency (RF) signals, or similar signals from sensors such as biomedical, temperature, etc., we are often interested in detecting certain signal “markers” or features. This can become a challenging problem when the signal-of-interest is degraded by noise. Traditional signal detection methods use a range of techniques such as energy detection, matched filtering, or other correlation-based processing techniques.\nFigure 1. Signal classification confusion matrix. Short-duration radio frequency (RF) events can be especially challenging to detect, since the useful data length is limited and long integration times are not possible. Weak signals that are short in duration are some of the most difficult to reliably detect (or even find). I will walk you through a simple approach using a Convolutional Neural Network (CNN) to tackle the traditional signal processing problem of detecting RF signals in noise. A little background information Signal detection theory often assumes that a signal is corupted with additive white Gaussian noise (AWGN). This type of noise is common in the real world and the assumption makes mathematical analysis tractable. The detection of a signal in noise depends on the signal duration, amplitude, and the corresponding noise process. This becomes more difficult if correlated noise, or interfering signals, are also in the same band as the signal you wish to detect.\nIn this tutorial, we will assume no a-priori information about the parameters for the signal-of-interest. As input to the Convolutional Neural Network, we will utilize spectrograms computed from simulated Radio Frequency (RF) data using a common Fast Fourier Transform (FFT) based method. Taking the input data into the frequency domain as time-frequency grams, which are 2D representations just like images, allows us to visualize the energy of a signal over some pre-determined time duration and frequency bandwidth.\nFigure 2. Multiple signals and noise (x-axis is time, y-axis is frequency). The difficulty with real-world signals\nFor a single sinusoid in AWGN, finding the frequency bin with the maximum amplitude is a method for estimating signal frequency in a spectrogram. But real-world signals are often more complex, with frequency components that change with time, and creating a generalized signal detection algorithm becomes difficult. In this tutorial, we will look at one of these types of signals - Linear Frequency-Modulated (LFM) signals. In a follow-on tutorial we will explore Frequency-Hopped (FH) signals and multi-signal detection scenarios. Linear Frequency-Modulated Signals\nA linear frequency-modulated (LFM), or chirp, signal is a signal that ramps up or down in frequency over some time frame. Its frequency changes with time based on its chirp rate. Chirps are used in many different systems for frequency response measurements and timing. RADAR systems use chirp signals due to the inherent large time-bandwith product available with coherent processing. Another common use is for automatic room equalization in home theater receivers, since chirps can excite a large frequency swath quickly. Chirps can also be used as “pilot” signals to denote the start of an incoming transmission, and more. Figure 3 shows a high-SNR chirp as seen in a grayscale spectrogram (the format we will be using). Since the spectrogram consists of real numbers all \u003e 0, we can map it to an image file by scaling the values appropriately. So we only need a single grayscale image channel. In this plot, the x axis is time and the y axis is frequency. Brightness is proportional to signal power.\nFigure 3. High-SNR chip spectrogram (grayscale). Figure 4. Weak chirp embedded in noise with other signals (x-axis is time, y-axis is frequency). Give it a try\nYou can try using deep learning yourself by following the link to my git page at the top of this post. There you will find a jupiter notebook. Modify the paths to point to the images of your time-frequency grams or reach out to me on LinkedIn and I will send you a dataset to use. When you are done you should have a trained model for classifying/detecting “chirps”, “CW”, and “Hopped” signal types. You can see a sample of my output results in Figure 5 below. Have fun!\nFigure 5. Example results from signal classifier. ","description":"Descrizione da rivedere se è un doppione subtitle","tags":["RF","ML","Deep Learning"],"title":"Deep Learning with Radio Frequency Signals","uri":"/deep-learning-with-radio-frequency-signals/"},{"content":"","description":"","tags":null,"title":"ML","uri":"/tags/ml/"},{"content":"","description":"","tags":null,"title":"RF","uri":"/tags/rf/"}]